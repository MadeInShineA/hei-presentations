---
title: "Spark I/O and File Layout: Partitioning Best Practices & Parquet Introduction"
format:
  revealjs:
    theme: sky
    transition: fade
    incremental: false
    slide-number: true
    preview-links: auto
    navigation-mode: grid
    footer: "Spark I/O Presentation | © 2025"
    logo: 
      path: "https://avatars.githubusercontent.com/u/108901596?s=200&v=4"
      width: 200
      height: 200
      placement: "top-left"
    css: ../../styles/302.1-data-computation/spark-io-presentation-style.scss
---


## Agenda {.agenda-slide}

1. **Spark I/O Basics**
2. **Partitioning Essentials**
3. **Performance Impact**
4. **Best Practices: Size & Strategies**
5. **Best Practices: Pruning & Pitfalls**
6. **Best Practices: Summary & Config**
7. **Parquet Introduction**
8. **Parquet Structure: Layers**
9. **Parquet Structure: Diagram**
10. **Key Takeaways**
11. **When to Use & Next Steps**
12. **Conclusion**

::: notes
Present the agenda to outline the structure. Highlight how we build from foundational concepts to practical tips and new tools like Parquet. Ask if the audience has prior Spark experience to gauge level.
:::

---

## Spark I/O Basics

Apache Spark I/O handles efficient data read/write across distributed systems.

**Key Aspects**:

- Distributed cluster processing
- Formats: CSV, JSON, Parquet, ORC
- Storage: HDFS, S3, local
- In-memory optimization

**Core Components**:

- DataSource API: Schema inference
- Partitioning: Parallelism
- Shuffling: Data redistribution

::: notes
Explain Spark I/O as the backbone for distributed data processing. Stress key aspects like formats and storage for real-world applicability. Transition to core components, noting how they enable scalability.
:::

---

## Spark I/O Pipeline

**Principles**:

- Partitioning: Parallel chunks
- Locality: Compute near data
- Fault Tolerance: Lineage
- Caching: Memory storage

```{mermaid}
flowchart LR
    A[Data Source] --> B[DataFrame]
    B --> C[Partition]
    C --> D[Process]
    D --> E[Shuffle]
    E --> F[Output]
    
    classDef input fill:#e1f5fe
    classDef process fill:#f3e5f5
    classDef output fill:#e8f5e8
    class A input
    class B,C,D process
    class E,F output
```

**Operations**:

- Read/Write: With partitioning
- Streaming: Real-time
- Connectors: Kafka, etc.

::: notes
Walk through the Mermaid diagram step-by-step, emphasizing partitioning's role in parallelism. Mention operations like read/write to connect theory to practice. Point out how caching reduces I/O bottlenecks.
:::

---

## Partitioning Essentials

**What & Why?**  
Divides data for parallel processing, boosting performance.

**Types**:

- Input: File splits (~128MB)
- Shuffle: Transformations
- Output: By columns/keys

**Strategies**:

- Range/Hash/File/Custom

**Mechanics**:

- Repartition/Coalesce
- Bucketing for joins
- Pruning: Skip irrelevant data

::: notes
Define partitioning simply: dividing data for parallel work. Discuss types and strategies, using examples like date-based partitioning. Warn about mechanics like repartition to avoid common errors.
:::

---

## Performance Impact

**Benefits**:

- Parallelism: Full cluster use
- Resource Efficiency: Even load

**Risks**:

- Too Many: Overhead
- Skew: Imbalance
- Memory: Overload

**Mitigation**:

- Query alignment
- Monitor skew

::: notes
Highlight benefits with a quick example of speedup. Discuss risks like skew, sharing a real-world story of imbalance causing delays. Introduce mitigation as teaser for best practices.
:::

---

## Best Practices: Size & Strategies

**Optimal Size**: 100MB–1GB per partition (balance overhead/memory).

**Strategies**:

- Filter columns (date/region)
- Avoid high cardinality
- File example: year/month/day

::: notes
Recommend optimal partition size with rationale: too small increases overhead, too large wastes resources. Give strategy examples like partitioning by region for geo-queries to illustrate pruning gains.
:::

---

## Best Practices: Pruning & Pitfalls

**Pruning**:

- Pushdown filters for I/O savings (50-90% reduction)
- Use in WHERE clauses on partition columns

**Pitfalls**:

- Too many: Metadata overhead
- Skew: Use salting
- Small files: Coalesce
- Deep nesting: Limits effectiveness

```{mermaid}
flowchart TD
    A["Pitfalls"] --> B["Too Many<br/>Partitions"]
    A --> C["Data Skew<br/>Imbalance"]
    A --> D["Small Files<br/>Waste"]
    
    classDef pitfall fill:#ffebee,stroke:#c62828
    class A,B,C,D pitfall
```

::: notes
Explain pruning with a before/after I/O savings example (e.g., scanning 10% of data). Use the diagram to visualize pitfalls; suggest salting for skew and coalescing for small files.
:::

---

## Best Practices: Summary & Config

**Core Guidelines**:

- Select columns wisely: Frequent filters
- Size appropriately: 100MB–1GB
- Keep shallow: 2-3 levels max
- Query-driven: Match patterns
- Monitor: Skew, files, times

**Key Configurations**:

| Parameter | Purpose | Rec. |
|-----------|---------|------|
| maxPartitionBytes | Input size | 100MB–1GB |
| shuffle.partitions | Shuffle | 200–2000 |
| adaptive.enabled | Dynamic | true |

::: notes
Summarize guidelines verbally, tying back to earlier slides. Walk through the config table, explaining each parameter's impact. Encourage audience to note these for their projects.
:::

---

## Parquet Introduction

**What is Parquet?**  
Columnar format for Spark/big data analytics.

**Why Use It?**:

- Columnar: Read specific columns
- Compression: 50-95% savings
- Pushdown: File-level filters
- Schema Evolution: Parquet supports backward/forward compatibility, allowing addition, deletion, or renaming of columns without rewriting entire datasets, ideal for evolving data pipelines.

**vs. Row Formats**:

| Feature | CSV/JSON | Parquet |
|---------|----------|---------|
| Storage | Large | Smaller |
| Speed | Slow | Fast |
| Schema | Loose | Strong |
| Opt. | Basic | Advanced |

::: notes
Introduce Parquet as a columnar powerhouse for analytics. Compare to row formats using the table to show advantages. Tease how it pairs with partitioning for max efficiency.
:::

---

## Parquet Structure: Layers

**Key Layers**:

1. Header: Magic bytes
2. Row Groups: 128MB–1GB chunks
3. Column Chunks: Per column
4. Pages: ~1MB units
5. Footer: Schema/stats

**Features**:

- Encoding: RLE/Dictionary
- Stats: Min/max for pruning
- Types: Nested support

::: notes
Describe each layer briefly, focusing on how row groups and pages support compression. Mention stats in footer for pruning, linking back to partitioning concepts.
:::

---

## Parquet Structure: Diagram

```{mermaid}
graph TD
    A[File] --> B[Header]
    A --> C[Row Groups]
    C --> D[Column 1]
    C --> E[Column 2]
    D --> F[Data Page]
    A --> G[Footer]
    
    classDef meta fill:#f3e5f5
    classDef data fill:#e8f5e8
    class B,G meta
    class C,D,E,F data
```

::: notes
Use the diagram to illustrate Parquet's structure. Point out how column chunks enable selective reading, reinforcing why it's ideal for analytical queries.
:::

---

## Key Takeaways

**Partitioning**:

- Size: 100MB–1GB
- Prune: Filter columns
- Avoid: Skew/high cardinality
- Monitor: Query patterns

**Parquet**:

- Columnar efficiency
- Compression gains
- 10-100x faster queries
- For analytics/large data

**Checklist**:

- Analyze patterns
- Optimize sizes/compression
- Test pruning
- Monitor performance

::: notes
Recap partitioning and Parquet points, using the checklist to reinforce action items. Ask the audience what they plan to implement first.
:::

---

## When to Use & Next Steps

**Use Cases**:

- Analytics/reporting
- Data lakes
- ETL with aggregations
- Evolving Schemas: Ideal for datasets where structure changes over time, as Parquet handles schema modifications efficiently without data loss or full reprocessing.

**Implementation**:

- Low-cardinality partitions
- Snappy compression
- Adaptive Spark enabled
- Iterate on metrics

::: notes
Discuss use cases with examples from industry (e.g., data lakes in e-commerce). Suggest next steps like experimenting with Parquet in their Spark jobs.
:::

---

## Conclusion

- **Partitioning**: Aim for 100MB–1GB partitions, use low-cardinality columns for pruning, monitor for skew.
- **Parquet**: Columnar storage for efficient analytics, compression, and schema evolution.

**Apply these practices to optimize your Spark workflows and achieve better performance!**

::: notes
Conclude by reiterating the core benefits. Open for Q&A: 'What questions do you have on partitioning or Parquet?'
:::

---

## Sources

**References**

- [Apache Spark Documentation](https://spark.apache.org/docs/latest/)
- [Apache Parquet](https://parquet.apache.org/)
- [Spark SQL Performance Tuning](https://spark.apache.org/docs/latest/sql-performance-tuning.html)

::: notes
Key sources for further reading on Spark I/O, partitioning, and Parquet.
:::

