---
title: "Spark I/O and File Layout: Partitioning Best Practices & Parquet Introduction"
format:
  revealjs:
    theme: sky
    transition: fade
    incremental: false
    slide-number: true
    preview-links: auto
    navigation-mode: grid
    footer: "Spark I/O Presentation | © 2025"
    logo: 
      path: "https://avatars.githubusercontent.com/u/108901596?s=200&v=4"
      width: 200
      height: 200
      placement: "top-left"
    css: ../../styles/302.1-data-computation/spark-io-presentation-style.scss
---

## Agenda {.agenda-slide}

1. **Spark I/O Basics**
2. **Partitioning Essentials**
3. **Performance Impact**
4. **Best Practices: Size & Strategies**
5. **Best Practices: Pruning & Pitfalls**
6. **Best Practices: Summary & Config**
7. **Parquet Introduction**
8. **Parquet Structure: Layers**
9. **Parquet Structure: Diagram**
10. **Parquet Data Example**
10. **Key Takeaways**
11. **When to Use & Next Steps**
12. **Conclusion**

::: notes
“Today, we’ll explore how to optimize Spark I/O through smart partitioning and 
leverage Parquet for high-performance analytics. We’ll start with the 
fundamentals of Spark I/O, dive into partitioning best practices—including 
sizing, pruning, and common pitfalls—and then introduce Apache Parquet: what 
it is, why it matters, and how its columnar structure boosts performance.

Before we begin—how many of you have used Spark for data processing or ETL? 
That’ll help me tailor the pace.”
:::

---

## Spark I/O Basics

Apache Spark I/O handles efficient data read/write across distributed systems.

**Key Aspects**:

- Distributed cluster processing
- Formats: CSV, JSON, Parquet, ORC
- Storage: HDFS, S3, local
- In-memory optimization

**Core Components**:

- DataSource API: Schema inference
- Partitioning: Parallelism
- Shuffling: Data redistribution

::: notes
“At its core, Spark I/O is how Spark reads from and writes to distributed 
storage—whether that’s HDFS, S3, or local files. It supports multiple formats 
like CSV, JSON, ORC, and especially Parquet.

What makes Spark powerful is its ability to infer schemas, split data into 
partitions for parallel processing, and redistribute data via shuffling when 
needed—all while keeping operations fault-tolerant through lineage tracking.”
:::

---

## Spark I/O Pipeline

**Principles**:

- Partitioning: Parallel chunks
- Locality: Compute near data
- Fault Tolerance: Lineage
- Caching: Memory storage

```{mermaid}
flowchart LR
    A[Data Source] --> B[DataFrame]
    B --> C[Partition]
    C --> D[Process]
    D --> E[Shuffle]
    E --> F[Output]
    
    classDef input fill:#e1f5fe
    classDef process fill:#f3e5f5
    classDef output fill:#e8f5e8
    class A input
    class B,C,D process
    class E,F output
```

**Operations**:

- Read/Write: With partitioning
- Streaming: Real-time
- Connectors: Kafka, etc.

::: notes
“Think of Spark I/O as a pipeline: data flows from a source into a DataFrame, 
gets partitioned across the cluster, processed in parallel, possibly shuffled 
for joins or aggregations, and finally written out.

The key principles here are partitioning for parallelism, data locality to 
minimize network traffic, fault tolerance via lineage, and caching to avoid 
re-reading expensive data.

This pipeline also supports streaming and integrations with systems like 
Kafka—but today, we’ll focus on batch workloads and file-based I/O.”
:::

---

## Partitioning Essentials

**What & Why?**  
Divides data for parallel processing, boosting performance.

**Types**:

- Input: File splits (~128MB)
- Shuffle: Transformations
- Output: By columns/keys

**Strategies**:

- Range/Hash/File/Custom

**Mechanics**:

- Repartition/Coalesce
- Bucketing for joins
- Pruning: Skip irrelevant data

::: notes
“Partitioning is the act of splitting data into chunks that can be processed 
in parallel. There are three main types:

Input partitions, created when reading files (usually ~128MB each),

Shuffle partitions, generated during transformations like joins,

And output partitions, which you control when writing data—often by columns 
like date or region.

Strategies include hash, range, or custom partitioning. And techniques like 
bucketing can optimize joins, while partition pruning lets Spark skip 
irrelevant files entirely—dramatically cutting I/O.”
:::

---

## Performance Impact

**Benefits**:

- Parallelism: Full cluster use
- Resource Efficiency: Even load

**Risks**:

- Too Many: Overhead
- Skew: Imbalance
- Memory: Overload

**Mitigation**:

- Query alignment
- Monitor skew

::: notes
“When done right, partitioning unlocks full cluster utilization and even 
resource distribution. But get it wrong, and you’ll face serious issues:

Too many small partitions create scheduling overhead,

Data skew leaves some tasks running long after others finish,

And oversized partitions can cause memory pressure or OOM errors.

The good news? These are avoidable—with the right strategies, which we’ll 
cover next.”
:::

---

## Best Practices: Size & Strategies

**Optimal Size**: 100MB–1GB per partition (balance overhead/memory).

**Strategies**:

- Filter columns (date/region)
- Avoid high cardinality
- File example: year/month/day

::: notes
“Aim for 100MB to 1GB per partition—this balances parallelism against task 
overhead.

Choose partition columns wisely: use low-cardinality, frequently filtered 
fields like country, year, or product_category.

Avoid high-cardinality keys like user_id—they create too many tiny partitions.

A classic pattern: s3://data/year=2024/month=06/day=15/—this enables 
efficient time-based pruning.”
:::

---

## Best Practices: Pruning & Pitfalls

**Pruning**:

- Pushdown filters for I/O savings (50-90% reduction)
- Use in WHERE clauses on partition columns

**Pitfalls**:

- Too many: Metadata overhead
- Skew: Use salting
- Small files: Coalesce
- Deep nesting: Limits effectiveness

```{mermaid}
flowchart TD
    A["Pitfalls"] --> B["Too Many<br/>Partitions"]
    A --> C["Data Skew<br/>Imbalance"]
    A --> D["Small Files<br/>Waste"]
    
    classDef pitfall fill:#ffebee,stroke:#c62828
    class A,B,C,D pitfall
```

::: notes
“Partition pruning is one of Spark’s superpowers. If your query filters on a 
partition column—say, WHERE year = 2024—Spark reads only those files, skipping 
90%+ of your data in many cases.

But watch out for pitfalls:

Too many partitions overwhelm the metadata layer,

Skewed data causes straggler tasks—fix with salting or AQE,

Small files waste resources—use coalesce() or compaction,

And deeply nested directories (e.g., 5+ levels) reduce pruning effectiveness.”
:::

---

## Best Practices: Summary & Config

**Core Guidelines**:

- Select columns wisely: Frequent filters
- Size appropriately: 100MB–1GB
- Keep shallow: 2-3 levels max
- Query-driven: Match patterns
- Monitor: Skew, files, times

**Key Configurations**:

| Parameter | Purpose | Rec. |
|-----------|---------|------|
| maxPartitionBytes | Input size | 100MB–1GB |
| shuffle.partitions | Shuffle | 200–2000 |
| adaptive.enabled | Dynamic | true |

::: notes
“To recap: design partitions around your query patterns, keep them 100MB–1GB, 
limit depth to 2–3 levels, and always monitor for skew or small files.

Key Spark configs to know:

spark.sql.files.maxPartitionBytes → controls input partition size 
(set to 256MB–1GB),

spark.sql.shuffle.partitions → start with 200–2000 based on cluster size,

And enable Adaptive Query Execution (spark.sql.adaptive.enabled=true)—it 
auto-tunes partitions and handles skew at runtime.”
:::

---

## Parquet Introduction

**What is Parquet?**  
Columnar format for Spark/big data analytics.

**Why Use It?**:

- Columnar: Read specific columns
- Compression: 50-95% savings
- Pushdown: File-level filters
- Schema Evolution: Parquet supports backward/forward compatibility, allowing addition, deletion, or renaming of columns without rewriting entire datasets, ideal for evolving data pipelines.

**vs. Row Formats**:

| Feature | CSV/JSON | Parquet |
|---------|----------|---------|
| Storage | Large | Smaller |
| Speed | Slow | Fast |
| Schema | Loose | Strong |
| Opt. | Basic | Advanced |

::: notes
“Now, let’s talk about Apache Parquet—the gold standard for analytical data 
in Spark.

Unlike row-based formats like CSV or JSON, Parquet stores data column by 
column. This means if your query only needs salary, Spark reads just that 
column—not the entire row.

Benefits? 50–95% smaller files, faster scans, predicate pushdown, and schema 
evolution—so you can safely add or rename columns over time without breaking 
pipelines.”
:::

---

## Parquet Structure: Layers

**Key Layers**:

1. Header: Magic bytes
2. Row Groups: 128MB–1GB chunks
3. Column Chunks: Per column
4. Pages: ~1MB units
5. Footer: Schema/stats

**Features**:

- Encoding: RLE/Dictionary
- Stats: Min/max for pruning
- Types: Nested support

::: notes
“A Parquet file is organized in layers:

It starts with a header,

Then one or more row groups (typically 128MB–1GB),

Each row group contains column chunks—one per column,

Which are further split into pages (~1MB) for fine-grained compression,

And ends with a footer that holds the schema and min/max statistics for every 
column.

These stats enable predicate pushdown: Spark can skip entire row groups if 
they don’t match your filter.”
:::

---

## Parquet Structure: Diagram

```{mermaid}
graph TD
    A[File] --> B[Header]
    A --> C[Row Groups]
    C --> D[Column 1]
    C --> E[Column 2]
    D --> F[Data Page]
    A --> G[Footer]
    
    classDef meta fill:#f3e5f5
    classDef data fill:#e8f5e8
    class B,G meta
    class C,D,E,F data
```

::: notes
“This diagram shows the hierarchy: a file contains row groups, each with 
column chunks broken into data pages. The footer holds metadata.

Because columns are stored separately, analytical queries that aggregate or 
filter a few columns become dramatically faster—and use far less I/O.”
:::

---

## Parquet Data Example

**Simple Dataset (Row-wise view):**

| ID | Name | Age | Salary |
|----|------|-----|--------|
| 1  | Alice| 30  | 50000  |
| 2  | Bob  | 25  | 45000  |
| 3  | Carol| 35  | 60000  |

**Parquet Storage (Columnar):**

- **ID Column Chunk:** [1, 2, 3] (with min=1, max=3 stats)
- **Name Column Chunk:** ["Alice", "Bob", "Carol"] (dictionary encoded)
- **Age Column Chunk:** [30, 25, 35] (RLE encoded)
- **Salary Column Chunk:** [50000, 45000, 60000] (compressed)

This allows reading only needed columns, e.g., just "Salary" for aggregation queries.

::: notes
“Imagine this simple table with ID, Name, Age, and Salary. In Parquet, each 
column is stored independently.

The ID column might store [1,2,3] with min=1, max=3.

Name uses dictionary encoding—storing unique strings once.

Age might use run-length encoding.

So when you run SELECT AVG(Salary), Spark reads only the Salary column 
chunk—ignoring everything else. That’s the power of columnar storage.”
:::

---

## Key Takeaways

**Partitioning**:

- Size: 100MB–1GB
- Prune: Filter columns
- Avoid: Skew/high cardinality
- Monitor: Query patterns

**Parquet**:

- Columnar efficiency
- Compression gains
- 10-100x faster queries
- For analytics/large data

**Checklist**:

- Analyze patterns
- Optimize sizes/compression
- Test pruning
- Monitor performance

::: notes
“To maximize Spark performance:

Partition smartly: 100MB–1GB, low-cardinality, aligned with queries,

Use Parquet: for compression, column pruning, and faster analytics,

Enable AQE: let Spark auto-fix skew and coalesce small partitions,

And always test: monitor I/O, task duration, and file counts.

Think of this as a checklist for your next data pipeline.”
:::

---

## When to Use & Next Steps

**Use Cases**:

- Analytics/reporting
- Data lakes
- ETL with aggregations
- Evolving Schemas: Ideal for datasets where structure changes over time, as Parquet handles schema modifications efficiently without data loss or full reprocessing.

**Implementation**:

- Low-cardinality partitions
- Snappy compression
- Adaptive Spark enabled
- Iterate on metrics

::: notes
“These techniques shine in data lakes, ETL pipelines, reporting workloads, and 
anywhere you have evolving schemas.

To get started:

Write your next dataset in Parquet with Snappy compression,

Partition by 1–2 meaningful dimensions,

Enable AQE,

And measure the difference in job duration and I/O.

You’ll likely see dramatic gains.”
:::

---

## Conclusion

- **Partitioning**: Aim for 100MB–1GB partitions, use low-cardinality columns for pruning, monitor for skew.
- **Parquet**: Columnar storage for efficient analytics, compression, and schema evolution.

**Apply these practices to optimize your Spark workflows and achieve better performance!**

::: notes
“In summary: thoughtful partitioning and columnar storage with Parquet are two 
of the highest-leverage optimizations in Spark. Together, they reduce I/O, 
accelerate queries, and scale efficiently.

Apply these practices, and you’ll build faster, more reliable data pipelines.

Now—I’d love your questions! What part are you most excited to try in your own 
work?”
:::

---

## Sources

**References**

- [Apache Spark Documentation](https://spark.apache.org/docs/latest/)
- [Apache Parquet](https://parquet.apache.org/)
- [Spark SQL Performance Tuning](https://spark.apache.org/docs/latest/sql-performance-tuning.html)

::: notes
“All the details are in the official docs—Spark’s performance tuning guide, 
Parquet’s site, and the main Spark documentation. I encourage you to explore 
them!”
:::
